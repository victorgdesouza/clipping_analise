import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import RSLPStemmer # Stemmer para portuguÃªs
import re
# Supondo que 'leIA' possa ser instalado ou esteja no seu diretÃ³rio
# Se 'leIA' for um mÃ³dulo local, certifique-se que estÃ¡ no PYTHONPATH
# ou na mesma pasta do app.py
try:
    from leia import SentimentIntensityAnalyzer
except ImportError:
    st.error("A biblioteca 'leIA' nÃ£o foi encontrada. Certifique-se de que estÃ¡ instalada ou no caminho correto.")
    # VocÃª pode tentar instalÃ¡-la se souber o nome do pacote PyPI
    # ou instruir o usuÃ¡rio a instalÃ¡-la.

# --- Downloads NLTK (executar apenas uma vez, se necessÃ¡rio) ---
@st.cache_resource
def inicializar_nltk():
    try:
        nltk.data.find('corpora/stopwords')
    except nltk.downloader.DownloadError:
        nltk.download('stopwords')
    try:
        nltk.data.find('tokenizers/punkt')
    except nltk.downloader.DownloadError:
        nltk.download('punkt')
    # O stemmer RSLP nÃ£o precisa de download de recurso separado, mas a tokenizaÃ§Ã£o sim.

inicializar_nltk()
stop_words_pt = stopwords.words('portuguese')
stemmer = RSLPStemmer()

# --- FunÃ§Ãµes de PrÃ©-processamento e AnÃ¡lise (adaptadas do seu notebook) ---

def limpar_texto(texto):
    if not isinstance(texto, str):
        return ""
    texto = texto.lower()
    texto = re.sub(r'http\S+', '', texto) # Remove URLs
    texto = re.sub(r'@\w+', '', texto) # Remove menÃ§Ãµes
    texto = re.sub(r'#\w+', '', texto) # Remove hashtags
    texto = re.sub(r'[^\w\s]', '', texto) # Remove pontuaÃ§Ã£o
    texto = re.sub(r'\d+', '', texto) # Remove nÃºmeros
    tokens = word_tokenize(texto, language='portuguese')
    tokens_limpos = [stemmer.stem(palavra) for palavra in tokens if palavra not in stop_words_pt and len(palavra) > 2]
    return " ".join(tokens_limpos)

def analisar_sentimento(texto):
    if not isinstance(texto, str) or not texto.strip():
        return None, "Neutro" # Retorna None para score se nÃ£o houver texto
    
    analyzer = SentimentIntensityAnalyzer()
    vs = analyzer.polarity_scores(texto)
    
    compound_score = vs['compound']
    if compound_score >= 0.05:
        return compound_score, "Positivo"
    elif compound_score <= -0.05:
        return compound_score, "Negativo"
    else:
        return compound_score, "Neutro"

def gerar_wordcloud(texto_processado, title="Word Cloud"):
    if not texto_processado.strip():
        return None
    wordcloud = WordCloud(width=800, height=400, background_color='white',
                          stopwords=stop_words_pt, # Reaplicar stopwords caso o texto jÃ¡ nÃ£o esteja 100% limpo para WC
                          collocations=True).generate(texto_processado)
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.imshow(wordcloud, interpolation='bilinear')
    ax.axis("off")
    plt.title(title)
    return fig

# --- Interface do Streamlit ---
st.set_page_config(layout="wide", page_title="AnÃ¡lise de Sentimento de Clipping")

st.title("ðŸ“Š AnÃ¡lise de Sentimento e Termos em Clippings de NotÃ­cias")
st.markdown("""
Esta ferramenta permite analisar o sentimento e os termos mais frequentes em notÃ­cias.
FaÃ§a o upload de um arquivo CSV contendo as notÃ­cias ou insira o texto de uma notÃ­cia diretamente.
O CSV deve conter uma coluna com o texto das notÃ­cias (especifique o nome da coluna abaixo).
""")

# --- Sidebar para Upload e ConfiguraÃ§Ãµes ---
st.sidebar.header("Fonte dos Dados")
tipo_entrada = st.sidebar.radio("Escolha a fonte dos dados:", ("Upload de Arquivo CSV", "Inserir Texto Ãšnico"))

df_noticias = None
textos_para_analise = [] # Lista para armazenar os textos

if tipo_entrada == "Upload de Arquivo CSV":
    arquivo_csv = st.sidebar.file_uploader("FaÃ§a upload do seu arquivo CSV", type=["csv"])
    if arquivo_csv is not None:
        try:
            df_original = pd.read_csv(arquivo_csv)
            st.sidebar.success(f"Arquivo '{arquivo_csv.name}' carregado com sucesso!")
            st.subheader("PrÃ©-visualizaÃ§Ã£o dos Dados Carregados")
            st.dataframe(df_original.head())

            colunas_disponiveis = df_original.columns.tolist()
            coluna_texto = st.sidebar.selectbox("Selecione a coluna contendo o texto das notÃ­cias:", colunas_disponiveis)

            if coluna_texto:
                # Verificar se a coluna selecionada existe e nÃ£o estÃ¡ vazia
                if coluna_texto in df_original.columns and not df_original[coluna_texto].isnull().all():
                    df_noticias = df_original.dropna(subset=[coluna_texto]).copy() # Remove linhas onde a coluna de texto Ã© NaN
                    textos_para_analise = df_noticias[coluna_texto].astype(str).tolist()
                else:
                    st.sidebar.error(f"A coluna '{coluna_texto}' estÃ¡ vazia ou nÃ£o contÃ©m textos vÃ¡lidos.")
                    textos_para_analise = []
            
        except Exception as e:
            st.sidebar.error(f"Erro ao ler o arquivo CSV: {e}")
            textos_para_analise = []

elif tipo_entrada == "Inserir Texto Ãšnico":
    texto_unico = st.text_area("Insira o texto da notÃ­cia aqui:", height=200)
    if texto_unico:
        textos_para_analise = [texto_unico]
        df_noticias = pd.DataFrame({'texto_original': [texto_unico]}) # Criar um DataFrame para consistÃªncia

# --- BotÃ£o de AnÃ¡lise ---
if st.button("ðŸš€ Iniciar AnÃ¡lise"):
    if not textos_para_analise:
        st.warning("Por favor, carregue um arquivo CSV com uma coluna de texto vÃ¡lida ou insira um texto para anÃ¡lise.")
    else:
        with st.spinner("Realizando a anÃ¡lise... Por favor, aguarde."):
            
            resultados_analise = []
            textos_processados_completos = []

            for i, texto_original in enumerate(textos_para_analise):
                st.progress((i + 1) / len(textos_para_analise), text=f"Processando notÃ­cia {i+1}/{len(textos_para_analise)}")
                texto_limpo = limpar_texto(texto_original)
                score_sentimento, sentimento_label = analisar_sentimento(texto_original) # Usar texto original para LeIA
                
                resultados_analise.append({
                    'texto_original': texto_original,
                    'texto_processado': texto_limpo,
                    'score_sentimento': score_sentimento,
                    'sentimento': sentimento_label
                })
                if texto_limpo: # Apenas adiciona se houver conteÃºdo apÃ³s limpeza
                    textos_processados_completos.append(texto_limpo)
            
            df_resultados = pd.DataFrame(resultados_analise)

            st.success("AnÃ¡lise concluÃ­da!")
            st.markdown("---")
            
            # --- ExibiÃ§Ã£o dos Resultados ---
            st.subheader("Resultados da AnÃ¡lise de Sentimento")
            if not df_resultados.empty:
                st.dataframe(df_resultados[['texto_original', 'sentimento', 'score_sentimento']].head())

                # Contagem de sentimentos
                contagem_sentimentos = df_resultados['sentimento'].value_counts()
                st.write("DistribuiÃ§Ã£o dos Sentimentos:")
                
                col1, col2 = st.columns(2)
                with col1:
                    st.dataframe(contagem_sentimentos)
                with col2:
                    if not contagem_sentimentos.empty:
                        fig_pie, ax_pie = plt.subplots()
                        contagem_sentimentos.plot(kind='pie', ax=ax_pie, autopct='%1.1f%%', startangle=90,
                                                  colors=[sns.color_palette("pastel")[i] for i in range(len(contagem_sentimentos))])
                        ax_pie.set_ylabel('') # Remove o label do eixo y
                        ax_pie.set_title("DistribuiÃ§Ã£o Percentual de Sentimentos")
                        st.pyplot(fig_pie)
                    else:
                        st.info("NÃ£o hÃ¡ dados de sentimento para exibir o grÃ¡fico de pizza.")

                # Download dos resultados
                @st.cache_data # Use st.cache_data para funÃ§Ãµes que retornam dados
                def converter_df_para_csv(df):
                    return df.to_csv(index=False).encode('utf-8')

                csv_resultados = converter_df_para_csv(df_resultados)
                st.download_button(
                    label="Download dos resultados da anÃ¡lise como CSV",
                    data=csv_resultados,
                    file_name='analise_sentimento_clipping.csv',
                    mime='text/csv',
                )
            else:
                st.info("Nenhum resultado de sentimento para exibir.")

            st.markdown("---")
            st.subheader("Word Cloud dos Termos Mais Frequentes")
            if textos_processados_completos:
                texto_completo_para_wc = " ".join(textos_processados_completos)
                if texto_completo_para_wc.strip(): # Verifica se hÃ¡ algo para gerar a nuvem
                    fig_wordcloud = gerar_wordcloud(texto_completo_para_wc, "Word Cloud Geral das NotÃ­cias Processadas")
                    if fig_wordcloud:
                        st.pyplot(fig_wordcloud)
                    else:
                        st.warning("NÃ£o foi possÃ­vel gerar a Word Cloud (texto processado pode estar vazio).")
                else:
                    st.info("NÃ£o hÃ¡ texto suficiente apÃ³s o processamento para gerar uma Word Cloud.")
            else:
                st.info("Nenhum texto processado disponÃ­vel para a Word Cloud.")

            # Opcional: Mostrar Word Clouds por sentimento
            st.markdown("---")
            st.subheader("Word Clouds por Sentimento (Opcional)")
            exibir_wc_sentimento = st.checkbox("Mostrar Word Clouds por categoria de sentimento?", value=False)

            if exibir_wc_sentimento and not df_resultados.empty:
                for sentimento_cat in df_resultados['sentimento'].unique():
                    if pd.notna(sentimento_cat): # Checa se nÃ£o Ã© NaN
                        textos_sentimento = " ".join(df_resultados[df_resultados['sentimento'] == sentimento_cat]['texto_processado'].dropna())
                        if textos_sentimento.strip():
                            st.write(f"**Word Cloud para notÃ­cias com sentimento: {sentimento_cat}**")
                            fig_wc_cat = gerar_wordcloud(textos_sentimento, f"Word Cloud - {sentimento_cat}")
                            if fig_wc_cat:
                                st.pyplot(fig_wc_cat)
                            else:
                                st.info(f"NÃ£o foi possÃ­vel gerar Word Cloud para '{sentimento_cat}'.")
                        else:
                            st.info(f"NÃ£o hÃ¡ texto processado suficiente para '{sentimento_cat}'.")


else:
    st.info("Aguardando dados e o comando para iniciar a anÃ¡lise.")

st.sidebar.markdown("---")
st.sidebar.markdown("Desenvolvido com base no projeto [clipping_analise](https://github.com/victorgdesouza/clipping_analise)")
